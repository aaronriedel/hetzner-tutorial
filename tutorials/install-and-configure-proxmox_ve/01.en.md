---
path: "/tutorials/install-and-configure-proxmox_ve"
slug: "install-and-configure-proxmox_ve"
date: "2021-09-12"
title: "Install and Configure Proxmox VE"
short_description: "This tutorial describes the basics of installing Proxmox VE, an open-source virtualization platform."
tags: ["Hetzner Official", "Proxmox"]
author: "Hetzner Online"
author_link: "https://github.com/hetzneronline"
author_img: "https://avatars3.githubusercontent.com/u/30047064?s=200&v=4"
author_description: ""
language: "en"
available_languages: ["en", "de"]
header_img: "header-4"
cta: "dedicated"
---

# **Introduction**

Proxmox Virtual Environment (Proxmox VE) is an open-source virtualization platform with support for Kernel-based Virtual Machine (KVM) and Linux Containers (LXC). Proxmox provides a web-based management interface, CLI tools, and REST API for management purposes as well as [a great documentation](https://pve.proxmox.com/pve-docs/index.html) including detailed Proxmox VE Administration Guide, manual pages and API viewer.

This tutorial shows how to install Proxmox VE 8 on Debian 12 and configure IP addresses on virtual machines.

## ***Before the Installation***

First, some suggestions and advice before starting to set up the new environment:

- Are only Linux machines going to be used? Then, under certain circumstances, LXC would be sufficient.
- Should LXC or KVM be used? Both have their advantages as well as disadvantages.

A thoughtful decision and good research can provide less work/trouble in the future.

- Although KVM is not as performant as LXC, it provides a complete hardware virtualization and enables the operation of all the most common operating systems (including Windows).

A conversion of the virtual disks in formats such as VMDK is simple.

## **Step 1 - Installation**

### **Step 1.1 - The Basic Installation on a Hetzner Server**

Boot the server into the [Rescue System](https://docs.hetzner.com/robot/dedicated-server/troubleshooting/hetzner-rescue-system/).

Run [installimage](https://docs.hetzner.com/robot/dedicated-server/operating-systems/installimage/), select and install the required Debian 12 (Bookworm).

Configure the RAID level, hostname, and partitioning.

Save the configuration and after completion of the installation perform a restart.

## **Step 1.2 - Adjust the APT Sources**

The next step is to add a key and to adjust the APT sources:

```Go
curl -o /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg http://download.proxmox.com/debian/proxmox-release-bookworm.gpg
```

```Go
echo "deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription" > /etc/apt/sources.list.d/pve-install-repo.list
```


If you do not have a Proxmox VE Enterprise subscription and wish to avoid unauthorized access errors, you can comment out the Proxmox VE Enterprise repository with the following command:

```Go
echo '# deb https://enterprise.proxmox.com/debian/pve bookworm InRelease' > /etc/apt/sources.list.d/pve-enterprise.list
```

Now update the packages:

```Go
apt update        # Update package lists
apt full-upgrade  # Update system
```

## **Step 1.3 - Install Proxmox VE**

```Go
apt install proxmox-ve
```
After a restart, the Proxmox kernel should be loaded. Run the following command in order to see kernel release information:

```Go
uname -r
```
The output should contain pve, for example: 

```Go
6.5.13-1-pve
```
Additionally, you can access the web interface by navigating to `https://<main address>:8006`.


## **Step 2 - Network Configuration**

First of all, it is important to decide which virtualization solution (LXC and/or KVM) and which variant (bridged/routed) will be used.

If you are not familiar with mentioned technologies, please find a simple comparison in the "Pros and Cons of xxx option" sections below.
Pros and Cons of LXC Option

- Advantages:
    * Lightweight, fast, lower RAM requirement.
        
    * Mixed fast startup and shutdown times, enhancing the manageability and scalability of containerized applications.
    * Allows for more containers to run simultaneously on the same hardware compared to virtual machines.

- Disadvantages:
    * The kernel of the host system is used
        
    * Only Linux distributions can be used

Pros and Cons of KVM Option

- Advantages:
    * Each VM operates with its own kernel, enhancing security and stability.
    * Almost any operating systems can be installed.
        
    * Can take advantage of hardware virtualization features (e.g., Intel VT-x and AMD-V) to improve performance.

- Disadvantages:
    * VMs require their own set of resources, including a full copy of the operating system, leading to increased consumption of CPU, RAM, and storage.

Pros and Cons of Routed Network Option

- Advantages:
    * Multiple single IP addresses and subnets can be used on one VM.

- Disadvantages:
    
    * Extra routes on the host are required.
    * Point-to-point setup is required.


Pros and Cons of Bridged Network Option

- Advantages:
    * The host is transparent and not part of the routing.
    * VMs can directly communicate with the gateway of the assigned IP.
    * VMs can get their single IPv4 address from Hetzner's DHCP server.

- Disadvantages:
    * VMs may only communicate via the MAC address assigned to the respective IP address.
    * That MAC address must be requested in Hetzner Robot.
    * IP addresses from additional subnets can only be used on the host system or on a single VM with a single IP (if the subnet is routed to it) (applies to both IPv4 and IPv6 subnets).


### **Enable IP Forwarding on the Host**
    
With a routed setup, the bridges (`vmbr0`, for example) are not connected with the physical interface. IP forwarding needs to be activated on the host system. Please note that packet forwarding between network interfaces is disabled for the default Hetzner installation. In order to make IP forwarding persisten across reboots, use the following commands bellow:

For IPv4:
```bash
sed -i 's/#net.ipv4.ip_forward=1/net.ipv4.ip_forward=1/' /etc/sysctl.conf
```
For IPv6:
    
```bash
sed -i 's/#net.ipv6.conf.all.forwarding=1/net.ipv6.conf.all.forwarding=1/' /etc/sysctl.conf
```
Now you can apply the changes with:

```bash
sysctl -p
```
Check if the forwarding is active:
    
```bash
sysctl net.ipv4.ip_forward
sysctl net.ipv6.conf.all.forwarding
```
    
## **Network configuration Host system Routed**
    

In a routed configuration, the host system's bridge IP address serves as the gateway by default, and manual route addition to a virtual machine is required when the extra IP does not belong to the same subnet. That's why we will set the mask to /32 because devices such as vmbr0 inherently route traffic only for IP addresses that fall within the same subnet.  By using a /32 mask, each additional IP is treated as its own unique network entity which will ensure the  that the traffic reaches its correct destination even if the additional IP is not from the same subnet.
    
Before we begin with the network configuration tutorial we will use the following addresses to represent a real-use-case scenario:

- ***Main IP*** - 198.51.100.10/24
- ***Gateway of the main IP*** - 198.51.100.1/24
- ***Additional Subnet*** - 203.0.113.0/24
- ***Additional single IP*** - 192.0.2.20/24
- ***IPv6*** - 2001:DB8::/64

```Go
# /etc/network/interfaces

auto lo
iface lo inet loopback

iface lo inet6 loopback

auto enp0s31f6
iface enp0s31f6 inet static
        address 198.51.100.10/32    #Main IP
        gateway 198.51.100.1        #Gateway

# IPv6 for the main interface
iface enp0s31f6 inet6 static
    address 2001:db8::2             # IPv6 address from the /64 subnet
    netmask 64
    gateway fe80::1

# Bridge for single IP's (foreign and same subnet)
auto vmbr0
iface vmbr0 inet static
        address 198.51.100.10/32     #Main IP
        bridge-ports none
        bridge-stp off
        bridge-fd 0
        up ip route add 192.0.2.20/32 dev vmbr0    # Foreign subnet
        up ip route add 198.51.100.30/32 dev vmbr0 # Additional IP from the same subnet

# IPv6 for the bridge
iface vmbr0 inet6 static
  address 2001:db8::3                # Should not be the same address as the main Interface
  netmask 64
  up ip -6 route add 2001:db8::/64 dev vmbr0

# Additional Subnet 203.0.113.0/24
auto vmbr1
iface vmbr1 inet static
        address 203.0.113.1/24 # Set one usable IP from the subnet range
        bridge-ports none
        bridge-stp off
        bridge-fd 0
```
### **Network configuration Guest system Routed (Debian 12)**

The IP of the bridge in the host system is always used as the gateway, i.e. the main IP for individual IPs and the IP from the subnet configured in the host system for subnets.

Guest configuration with a additional IP from the same subnet

```Go
# /etc/network/interfaces

auto lo
iface lo inet loopback


auto ens18
iface ens18 inet static
  address 198.51.100.30/32     # Additional IP
  gateway 198.51.100.10        # Main IP
    
# IPv6
iface ens18 inet6 static
  address 2001:DB8::4      # IPv6 address of the subnet
  netmask 64               # /64
  gateway 2001:DB8::3      # Bridge Address
```

Guest configuration with a foreign Additional IP

```Go
# /etc/network/interfaces

auto lo
iface lo inet loopback


auto ens18
iface ens18 inet static
  address 192.0.2.20/32        # Additional IP from a foreign subnet
  gateway 198.51.100.10        # Main IP   
```
Guest configuration with an IP assigned from the additional subnet

```Go
# /etc/network/interfaces

auto lo
iface lo inet loopback


auto ens18
iface ens18 inet static
  address 203.0.113.10/24      # Subnet IP
  gateway 203.0.113.1          # Gateway is the IP of the bridge (vmbr1) 
```

## **Network configuration Host system Bridged**

When setting up Proxmox in bridged mode, it is absolutely crucial to request virtual MAC addresses for each IP address through the Robot Panel. In this mode, the host acts as a transparent bridge and is not part of the routing path. This means that packets arriving at the router will have the source MAC address of the virtual machines. If the source MAC address is not recognized by the router, the traffic will be flagged sa "Abuse" and might lead to the server being blocked. Therefore, it is essential to request virtual MAC addresses in the Robot Panel.

```Go
# /etc/network/interfaces

auto lo
iface lo inet loopback

auto enp0s31f6
iface enp0s31f6 inet manual

auto vmbr0
iface vmbr0 inet static
        address 198.51.100.10/32    # Main IP
        gateway 198.51.100.1        # Gateway
        bridge-ports enp0s31f6
        bridge-stp off
        bridge-fd 0
```
We configure only the Main-IP of the server here. The additional IPs will be configured in the Guest Systems.

### **Network configuration Guest system Bridged (Debian 12)**

Here we use the gateway of the additional IP, or if the additional IP is within the same subnet as the main IP, we use the gateway of the main IP.

Static configuration:
```Go
# /etc/network/interfaces

auto ens18
iface ens18 inet static
  address 192.0.2.20/32         # Additional IP 
  gateway 192.0.2.1             # Additional Gateway IP  
```
In bridged mode, DHCP can also be used to automatically configure the network settings. However, it is essential to configure the virtual machine to use the virtual MAC address obtained from the Robot Panel for the specific IP configuration. 

![](images/d5ce2092-8887-4695-9bd1-ba9e3c8e3f71.png)

You can also adjust this manually in the Virtual Machine itself, in `/etc/network/interfaces`:

```Go
# /etc/network/interfaces

auto lo
iface lo inet loopback


auto ens18
iface ens18 inet dhcp
        hwaddress ether aa:bb:cc:dd:ee:ff # The MAC address is just an example
```
The same thing can be done for LXC containers via the Proxmox GUI, by simply clicking on the container, navigate to "Network" and then click on the bridge. Select DHCP and add the correct MAC address from the Robot Panel (in our case the example would be `aa:bb:cc:dd:ee:ff`):

![](images/e569dbf9-2431-4973-abad-d950b6b75eda.png)



## **Assigning IP addresses to VM's/Containers from a vSwitch with a public subnet**

Proxmox can also be set up to link directly with a Hetzner vSwitch that manages routing for a public subnet, allowing IP addresses from that subnet to be assigned directly to VM's and containers. The setup has to be a bridged setup and a virtual interfaces has to be created in order for the packets to be able to reach the vSwitch. The bridge does not require to be vlan-aware and no vlan configuration has to be made within the VM or LXC containers, the taging is here done by the subinterface, in our example the `enp0s31f6.4009`. Every packet that goes through the interface will be tagged with the appropriate vlan ID. (Please note that this configuration is meant for the LXC/VM's, If you would like the host itself to be able to communicate with the vSwitch you will have to create an additional [Routing table](https://docs.hetzner.com/robot/dedicated-server/network/vswitch#server-configuration-linux).) In this case we will use **203.0.113.0/24** as an example subnet.

```Bash
# /etc/network/interfaces

auto enp0s31f6.4009
iface enp0s31f6.4009 inet manual

auto vmbr4009
iface vmbr4009 inet static
        bridge-ports enp0s31f6.4009
        bridge-stp off
        bridge-fd 0
        mtu 1400
#vSwitch Subnet 203.0.113.0/24
```
### Guest system configuration

```Go 
# /etc/network/interfaces

auto lo
iface lo inet loopback


auto ens18
iface ens18 inet static
  address 203.0.113.2/24      # Subnet IP from the vSwitch
  gateway 203.0.113.1         # vSwitch Gateway
```

## Connecting Proxmox to the Hetzner Cloud

It is also possible to establish a connection between the virtual machines/LXC in Proxmox with the Hetzner Cloud Network. For the sake of this example, lets assume that you have already setup your cloud network and have added the vSwitch with the following configuration:

- ***192.168.0.0/16*** - Your cloud network (parent network)
- ***192.168.0.0/24*** - Cloud server subnet
- ***192.168.1.0.0/24*** - vSwitch (#12345)

The configuration should look something like this:

![](images/3935fcac-0bc7-47c1-8965-511f11d7f72c.png)

Similarly as in the example before, we will first create a virtual interface and define the vlan ID, so in our case that would be the **enp0s31f6.4000**. We will have to add the route to the Cloud Network **192.168.0.0/16** via the vSwitch. 

```Go
# /etc/network/interfaces

auto enp0s31f6.4000
iface enp0s31f6.4000 inet manual

auto vmbr4000
iface vmbr4000 inet static
        bridge-ports enp0s31f6.4000
        bridge-stp off
        bridge-fd 0
        mtu 1400
        up ip route add 192.168.0.0/16 via 192.168.1.1 dev vmbr4000
        
#vSwitch-to-cloud Private Subnet 192.168.1.0/24
```

### Guest system configuration

```Go
# /etc/network/interfaces

auto lo
iface lo inet loopback


auto ens18
iface ens18 inet static
  address 192.168.1.2/24
  gateway 192.168.1.1
```
### **Masquerading (NAT) with iptables**

Exposing the virtual machines/LXC containers to the internet is also possible without configuring/having any further public additional IP addresses.  Hetzner has a strict IP/MAC binding which means that if the traffic is not routed properly it will cause abuse and can lead to server lockings. In order to avoid this problem we can route the traffic from the LXC/VM's through the main interface of the host. This ensures that the MAC address is the same across all network packets. Masquerading enables virtual machines with private IP addresses to have internet access through the host's public IP address for outbound communications. Iptables modifies each outgoing data packet to seem as if it is coming from the host and incoming replies are adjusted so they can be directed back to the initial sender.
```Go
# /etc/network/interfaces

auto lo
iface lo inet loopback

iface lo inet6 loopback

auto enp0s31f6
iface enp0s31f6 inet static
        address 198.51.100.10/24
        gateway 198.51.100.1/24
        #post-up iptables -t nat -A PREROUTING -i enp0s31f6 -p tcp -m multiport ! --dports 22,8006 -j DNAT --to 172.16.16.2
        #post-down iptables -t nat -D PREROUTING -i enp0s31f6 -p tcp -m multiport ! --dports 22,8006 -j DNAT --to 172.16.16.2


auto vmbr4
iface vmbr4 inet static
        address 172.16.16.1/24
        bridge-ports none
        bridge-stp off
        bridge-fd 0
        post-up   iptables -t nat -A POSTROUTING -s '172.16.16.0/24' -o enp0s31f6 -j MASQUERADE
        post-down iptables -t nat -D POSTROUTING -s '172.16.16.0/24' -o enp0s31f6 -j MASQUERADE
#NAT/Masq
```
Please note that these rules (bellow) are not necessary for the LXC/VM's to have internet access. This rule is optional and serves the purpose of external accessibility to a specific VM/Container. It redirects all incoming traffic, besides on ports 22 and 8006 (22 is here excluded so you can still connect to Proxmox via SSH and 8006 is the port for the webinterface), to a designated virtual machine at 172.16.16.2 within the subnet. This is a common scenario/setup for router VM's like Pfsense where all incoming traffic will be redirected to the router VM and then routed accordingly.
```Go
post-up iptables -t nat -A PREROUTING -i enp0s31f6 -p tcp -m multiport ! --dports 22,8006 -j DNAT --to 172.16.16.1/24
post-down iptables -t nat -D PREROUTING -i enp0s31f6 -p tcp -m multiport ! --dports 22,8006 -j DNAT --to 172.16.16.1/24

```

## Step 3 - Security

The web interface is protected by two different authentication methods: Proxmox VE standard authentication (Proxmox proprietary authentication) and Linux PAM standard authentication.

Nevertheless, additional protection measures would be recommended to protect against the exploitation of any security vulnerabilities or various other attacks.

Here are several possibilities:

- [Two-Factor-Authentication](https://pve.proxmox.com/wiki/Two-Factor_Authentication)
- [Fail2ban against Brute-force attacks](https://pve.proxmox.com/wiki/Fail2ban)
- [Securing the SSH Service](https://community.hetzner.com/tutorials/securing-ssh)

### Conclusion

By now, you should have installed and configured Proxmox VE as a virtualization platform on your server.

Proxmox VE also support clustering. Please find details in [tutorial "Setting up your own public cloud with Proxmox on Hetzner bare metal"](https://community.hetzner.com/tutorials/hyperconverged-proxmox-cloud).
